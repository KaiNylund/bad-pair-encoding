{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"translate English to German: \"\n",
    "def preprocess(sample, tokenizer):\n",
    "    inputs = prefix + str(sample[\"0\"])\n",
    "    targets = str(sample[\"1\"])\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\").to(\"cuda\")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "ende_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-German_tokenizer\")\n",
    "enru_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-Russian_tokenizer\")\n",
    "enfi_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-Finnish_tokenizer\")\n",
    "\n",
    "ende_mtokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/gpt2_English-German_morpheme_tokenizer\")\n",
    "enru_mtokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/gpt2_English-Russian_morpheme_tokenizer\")\n",
    "enfi_mtokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/gpt2_English-Finnish_morpheme_tokenizer\")\n",
    "#if args.tokenizer != args.model:\n",
    "model_tokenizer_vocab = model_tokenizer.get_vocab()\n",
    "ende_tokenizer_vocab = ende_tokenizer.get_vocab()\n",
    "enru_tokenizer_vocab = enru_tokenizer.get_vocab()\n",
    "enfi_tokenizer_vocab = enfi_tokenizer.get_vocab()\n",
    "\n",
    "ende_mtokenizer_vocab = ende_mtokenizer.get_vocab()\n",
    "enru_mtokenizer_vocab = enru_mtokenizer.get_vocab()\n",
    "enfi_mtokenizer_vocab = enfi_mtokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/German/dev-small\"\n",
    "de_pd = pd.read_csv(de_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "de_dataset = Dataset.from_pandas(de_pd)\n",
    "\n",
    "ru_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/Russian/dev-small\"\n",
    "ru_pd = pd.read_csv(ru_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "ru_dataset = Dataset.from_pandas(ru_pd)\n",
    "\n",
    "fi_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/Finnish/dev-small\"\n",
    "fi_pd = pd.read_csv(fi_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "fi_dataset = Dataset.from_pandas(fi_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      Michael Ruse (2014), Philosophy of Science 81(...   \n",
      "1            Category: Category Member Since: 2009-10-19   \n",
      "2         Find your way through the world and to Gerard.   \n",
      "3      Nawang hasn’t written anything in his profile ...   \n",
      "4      Product details Tomatin three bridges - Ask a ...   \n",
      "...                                                  ...   \n",
      "24240  Wilde & Partner offers customized services to ...   \n",
      "24241  It is perfect for indoor and outdoor wear. You...   \n",
      "24242  Step 2: Install and launch 5K Player free YouT...   \n",
      "24243  I can’t really do much about it, but it helps ...   \n",
      "24244  The 10 best mechanical engineering teachers in...   \n",
      "\n",
      "                                                       1  \n",
      "0      Zeitschrift für philosophische Forschung 69 (2...  \n",
      "1           Erobere den ersten Mitglied seit: 2012-03-04  \n",
      "2      Durchlaufe die Welt und finde deinen Weg zum G...  \n",
      "3       Nisha hat noch nichts in her Profil geschrieben.  \n",
      "4      Produktdetails Mike Marshall & The Turtle Isla...  \n",
      "...                                                  ...  \n",
      "24240  Wilde & Partner Communications betreut seine K...  \n",
      "24241  Sie eignet sich sowohl als In- wie Outdoorjack...  \n",
      "24242  Schritt 2: Installieren und starten Sie den ko...  \n",
      "24243  Ich kann nicht wirklich viel dagegen tun, aber...  \n",
      "24244  10 besten Maschinenbau-Lehrer(n) in Otterbach,...  \n",
      "\n",
      "[24245 rows x 2 columns]\n",
      "Dataset({\n",
      "    features: ['0', '1'],\n",
      "    num_rows: 24338\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(de_pd)\n",
    "print(fi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193289\n",
      "189749\n",
      "198776\n",
      "226116\n",
      "211413\n"
     ]
    }
   ],
   "source": [
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(ende_tokenizer_vocab.keys()))))\n",
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(enru_tokenizer_vocab.keys()))))\n",
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(enfi_tokenizer_vocab.keys()))))\n",
    "\n",
    "print(len(set(enru_mtokenizer_vocab.keys()).difference(set(enru_tokenizer_vocab.keys()))))\n",
    "print(len(set(enfi_mtokenizer_vocab.keys()).difference(set(enfi_tokenizer_vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Kommt wieder, wann immer ihr wollt.“ Wow, wir waren wirklich sehr stolz auf dieses Feedback.\n",
      "['▁Komm', 't', '▁wieder', ',', '▁', 'wann', '▁immer', '▁', 'ihr', '▁woll', 't', '.“', '▁Wow', ',', '▁wir', '▁', 'waren', '▁w', 'irklich', '▁', 'sehr', '▁stol', 'z', '▁auf', '▁diese', 's', '▁Feedback', '.']\n",
      "['Kommt', '▁wieder', ',', '▁wann', '▁immer', '▁ihr', '▁wollt', '.“', '▁Wow', ',', '▁wir', '▁waren', '▁wirklich', '▁sehr', '▁stolz', '▁auf', '▁dieses', '▁Feedback', '.']\n",
      "['▁Komm', 't', '▁wieder', ',', '▁', 'wann', '▁immer', '▁', 'ihr', '▁woll', 't', '.“', '▁Wow', ',', '▁wir', '▁', 'waren', '▁w', 'irklich', '▁', 'sehr', '▁stol', 'z', '▁auf', '▁diese', 's', '▁Feedback', '.']\n"
     ]
    }
   ],
   "source": [
    "example_num = 115\n",
    "\n",
    "print(\"Sample: \" + de_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(de_dataset[example_num][\"1\"]))\n",
    "de_tokens = ende_tokenizer.tokenize(de_dataset[example_num][\"1\"])\n",
    "#print(f\"German-specific tokenization: {de_tokens}\")\n",
    "#print(de_tokens)\n",
    "\n",
    "de_mtokens = ende_mtokenizer.tokenize(de_dataset[example_num][\"1\"])\n",
    "de_mtokens = [mtok.replace(\"Ġ\", '▁').replace(\"Ã¤\", \"ä\").replace(\"âĢľ\", \"“\") for mtok in de_mtokens]\n",
    "print(de_mtokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(de_tokens)):\n",
    "    if de_tokens[i] not in model_tokenizer_vocab:\n",
    "        def_de_tok = model_tokenizer.tokenize(de_tokens[i])\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if de_tokens[i][0] != '▁' and def_de_tok[0] == '▁':\n",
    "            def_de_tok = def_de_tok[1:]\n",
    "        elif def_de_tok[0][0] == '▁' and re_tokenized[-1] == '▁':\n",
    "            del re_tokenized[-1]\n",
    "        re_tokenized += def_de_tok\n",
    "    else:\n",
    "        re_tokenized.append(de_tokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "#print(f\"German and then default tokenized: {re_tokenized}\")\n",
    "#print(preprocess(dev_dataset[example_num], model_tokenizer))\n",
    "#print(preprocess(dev_dataset[example_num], ende_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Joulunodotus käynnistyy Vanhan Suurtorin Joulumarkkinoilta\n",
      "['▁Joulu', 'no', 'dotus', '▁', 'käynnisty', 'y', '▁Van', 'han', '▁Suur', 'tor', 'in', '▁Joulu', 'markkinoi', 'lta']\n",
      "['▁Joulun', 'odotus', '▁käynnisty', 'y', '▁Vanhan', '▁Suur', 'torin', '▁Joulu', 'markkinoilta']\n",
      "['Joulun', 'odotus', '▁käynnistyy', '▁Vanhan', '▁Su', 'urt', 'orin', '▁Joul', 'umarkkin', 'oilta']\n",
      "['▁Joulu', 'n', '▁odot', 'us', '▁', 'käynnisty', 'y', '▁Van', 'han', '▁Su', 'ur', 't', 'orin', '▁Jo', 'ul', 'u', 'mark', 'kin', '▁oil', 'ta']\n"
     ]
    }
   ],
   "source": [
    "example_num = 2300\n",
    "\n",
    "print(\"Sample: \" + fi_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(fi_dataset[example_num][\"1\"]))\n",
    "fi_tokens = enfi_tokenizer.tokenize(fi_dataset[example_num][\"1\"])\n",
    "print(fi_tokens)\n",
    "\n",
    "fi_mtokens = enfi_mtokenizer.tokenize(fi_dataset[example_num][\"1\"])\n",
    "fi_mtokens = [mtok.replace(\"Ġ\", '▁') for mtok in fi_mtokens]\n",
    "print(fi_mtokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(fi_mtokens)):\n",
    "    if fi_mtokens[i] not in model_tokenizer_vocab:\n",
    "        def_fi_tok = model_tokenizer.tokenize(fi_mtokens[i], add_special_tokens=False)\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if len(def_fi_tok) > 0:\n",
    "            if fi_mtokens[i][0] != '▁' and def_fi_tok[0] == '▁':\n",
    "                def_fi_tok = def_fi_tok[1:]\n",
    "            elif def_fi_tok[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                del re_tokenized[-1]\n",
    "            re_tokenized += def_fi_tok\n",
    "    else:\n",
    "        re_tokenized.append(fi_mtokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "#print(preprocess(fi_dataset[example_num], model_tokenizer)[\"labels\"])\n",
    "#print(preprocess(fi_dataset[example_num], enfi_tokenizer)[\"labels\"])\n",
    "#print(preprocess(fi_dataset[example_num], enfi_mtokenizer)[\"labels\"])\n",
    "#print(model_tokenizer.convert_tokens_to_ids(re_tokenized) + [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Иоанн Возлюбленный Учение о правильном использовании Слов, 4 августа 2010 г.\n",
      "['▁Ио', 'ан', 'н', '▁Воз', 'люблен', 'ный', '▁У', 'чение', '▁', 'о', '▁правильн', 'ом', '▁использовани', 'и', '▁Слов', ',', '▁4', '▁август', 'а', '▁2010', '▁г', '.']\n",
      "['▁Иоанн', '▁Возлюбленны', 'й', '▁Учени', 'е', '▁', 'о', '▁', 'правильном', '▁использовании', '▁Слов', ',', '▁4', '▁августа', '▁2010', '▁г.']\n",
      "['ÐĺÐ¾Ð°Ð½Ð½', '▁ÐĴÐ¾Ð·Ð»ÑİÐ±Ð»ÐµÐ½Ð½ÑĭÐ¹', '▁Ð£ÑĩÐµÐ½Ð¸Ðµ', '▁Ð¾', '▁Ð¿ÑĢÐ°Ð²Ð¸Ð»ÑĮÐ½Ð¾Ð¼', '▁Ð¸ÑģÐ¿Ð¾Ð»ÑĮÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸', '▁Ð¡Ð»Ð¾Ð²', ',', '▁4', '▁Ð°Ð²Ð³ÑĥÑģÑĤÐ°', '▁2010', '▁Ð³', '.']\n",
      "['▁Ио', 'ан', 'н', '▁Воз', 'люблен', 'ны', 'й', '▁У', 'чени', 'е', '▁', 'о', '▁правильн', 'ом', '▁использовани', 'и', '▁Слов', ',', '▁4', '▁август', 'а', '▁2010', '▁г', '.']\n",
      "{'input_ids': [37194, 5413, 288, 20567, 267, 4040, 287, 1071, 28387, 259, 104683, 351, 287, 4645, 2225, 304, 15758, 261, 419, 3155, 1068, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [152395, 2057, 686, 90138, 103663, 1626, 867, 28888, 259, 411, 63295, 637, 20084, 279, 98472, 261, 419, 18648, 308, 1068, 1352, 260, 1]}\n",
      "{'input_ids': [15639, 1283, 11, 1783, 16, 2435, 5, 3, 36107, 19005, 29, 5, 438, 216, 8, 6089, 4, 123, 1722, 899, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [9006, 54682, 24, 38362, 15, 3, 41, 3, 33225, 3581, 72507, 4, 123, 3484, 899, 342, 1]}\n",
      "{'input_ids': [93409, 5504, 292, 7016, 26, 7730, 228, 78962, 46090, 384, 228, 1789, 1286, 253, 13049, 12, 728, 6501, 3456], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [73299, 161117, 94496, 251, 46023, 10158, 11317, 12, 728, 9964, 3456, 344, 14]}\n"
     ]
    }
   ],
   "source": [
    "example_num = 10\n",
    "\n",
    "print(\"Sample: \" + ru_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(ru_dataset[example_num][\"1\"]))\n",
    "fi_tokens = enru_tokenizer.tokenize(ru_dataset[example_num][\"1\"])\n",
    "print(fi_tokens)\n",
    "\n",
    "fi_mtokens = enru_mtokenizer.tokenize(ru_dataset[example_num][\"1\"])\n",
    "fi_mtokens = [mtok.replace(\"Ġ\", '▁').replace(\"Ã¤\", \"ä\") for mtok in fi_mtokens]\n",
    "print(fi_mtokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(fi_tokens)):\n",
    "    if fi_tokens[i] not in model_tokenizer_vocab:\n",
    "        def_fi_tok = model_tokenizer.tokenize(fi_tokens[i], add_special_tokens=False)\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if len(def_fi_tok) > 0:\n",
    "            if fi_tokens[i][0] != '▁' and def_fi_tok[0] == '▁':\n",
    "                def_fi_tok = def_fi_tok[1:]\n",
    "            elif def_fi_tok[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                del re_tokenized[-1]\n",
    "            re_tokenized += def_fi_tok\n",
    "    else:\n",
    "        re_tokenized.append(fi_tokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "print(preprocess(ru_dataset[example_num], model_tokenizer))\n",
    "print(preprocess(ru_dataset[example_num], enru_tokenizer))\n",
    "print(preprocess(ru_dataset[example_num], enru_mtokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Как', '▁правил', 'о', ',', '▁', 'наши', '▁', 'соо', 'течественн', 'ики', '▁хот', 'ят', '▁у', 'слыш', 'ать', '▁', 'еще', '▁одно', '▁мне', 'ние', '▁', 'авторитет', 'ного', '▁врач', 'а', ',', '▁пере', 'под', 'тверди', 'ть', '▁диагно', 'з', ':', '▁«', 'К', 'стати', ',', '▁в', '▁Германи', 'и', '▁за', 'част', 'ую', '▁с', 'мяг', 'чают', '▁диагно', 'з', ',', '▁предлага', 'ют', '▁мало', 'ин', 'ваз', 'ивное', '▁', 'лечение', '.']\n"
     ]
    }
   ],
   "source": [
    "print(model_tokenizer.tokenize(ru_dataset[example_num][\"1\"], max_length=128, truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize(tokens, new_tokenizer, new_tokenizer_vocab):\n",
    "    re_tokenized = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in new_tokenizer_vocab:\n",
    "            #print(tokens[i])\n",
    "            new_toks = new_tokenizer.tokenize(tokens[i], add_special_tokens=False)\n",
    "            #print(def_de_tok, de_tokens[i])\n",
    "            if len(new_toks) > 0:\n",
    "                if tokens[i][0] != '▁' and new_toks[0] == '▁':\n",
    "                    new_toks = new_toks[1:]\n",
    "                elif new_toks[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                    del re_tokenized[-1]\n",
    "                re_tokenized += new_toks\n",
    "        else:\n",
    "            re_tokenized.append(tokens[i])\n",
    "    re_tokenized = (new_tokenizer.convert_tokens_to_ids(re_tokenized) + [1])\n",
    "    return re_tokenized\n",
    "\n",
    "\n",
    "def lang_preprocess(sample):\n",
    "    inputs = prefix + str(sample[\"0\"])\n",
    "    targets = str(sample[\"1\"])\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"input_ids\"] = ende_tokenizer.tokenize(inputs, max_length=128, truncation=True)\n",
    "    model_inputs[\"input_ids\"] = retokenize(model_inputs[\"input_ids\"], model_tokenizer, set(model_tokenizer.get_vocab().keys()))\n",
    "    labels = model_tokenizer.tokenize(targets, max_length=128, truncation=True)\n",
    "    labels = retokenize(labels, model_tokenizer, model_tokenizer_vocab)\n",
    "    model_inputs[\"attention_mask\"] = ([1] * len(model_inputs[\"input_ids\"]))\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    #print(model_inputs)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [37194, 5413, 288, 20567, 267, 642, 262, 75443, 287, 1546, 3415, 304, 26594, 259, 108880, 261, 287, 259, 57611, 2178, 259, 53567, 2178, 259, 128868, 511, 19532, 259, 73433, 263, 15848, 287, 259, 54148, 23317, 259, 4964, 259, 159713, 305, 10385, 332, 259, 4609, 151738, 36732, 14679, 2178, 3399, 16185, 5233, 5927, 3826, 46213, 21511, 345, 514, 381, 403, 14181, 288, 259, 38195, 26594, 484, 264, 204941, 265, 25997, 305, 10239, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 22496, 259, 14416, 21638, 262, 6271, 286, 68041, 44050, 270, 309, 259, 57611, 2178, 259, 53567, 2178, 259, 106177, 511, 489, 264, 220805, 68041, 66934, 259, 45200, 272, 398, 42951, 9933, 259, 112444, 795, 259, 217071, 472, 58147, 749, 36732, 14679, 2178, 3399, 90849, 956, 67693, 119787, 3399, 264, 638, 50219, 28333, 472, 381, 403, 264, 174218, 956, 35834, 115806, 30966, 472, 259, 184667, 260, 1]}\n",
      "{'input_ids': [37194, 5413, 288, 20567, 267, 415, 294, 75443, 287, 1546, 259, 295, 262, 304, 26594, 259, 108880, 261, 287, 259, 57611, 2178, 259, 53567, 2178, 774, 5388, 511, 19532, 259, 73433, 263, 15848, 287, 259, 54148, 23317, 259, 4964, 259, 159713, 305, 10385, 332, 527, 270, 151738, 36732, 14679, 2178, 3399, 16185, 5233, 5927, 3826, 46213, 21511, 345, 514, 381, 403, 14181, 288, 259, 38195, 26594, 484, 264, 12854, 265, 25997, 305, 10239, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 22496, 259, 14416, 21638, 262, 6271, 286, 68041, 44050, 270, 309, 259, 57611, 2178, 259, 53567, 2178, 259, 106177, 511, 489, 264, 220805, 68041, 66934, 259, 45200, 272, 398, 42951, 9933, 259, 112444, 795, 259, 217071, 472, 58147, 749, 36732, 14679, 2178, 3399, 90849, 956, 67693, 119787, 3399, 264, 638, 50219, 28333, 472, 381, 403, 264, 174218, 956, 35834, 115806, 30966, 472, 259, 184667, 260, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_num = 7\n",
    "print(preprocess(de_dataset[example_num], model_tokenizer))\n",
    "print(lang_preprocess(de_dataset[example_num]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
