{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"translate English to German: \"\n",
    "def preprocess(sample, tokenizer):\n",
    "    inputs = prefix + str(sample[\"0\"])\n",
    "    targets = str(sample[\"1\"])\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/mmfs1/gscratch/ark/knylund/.conda/envs/arkenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\").to(\"cuda\")\n",
    "model_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "ende_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-German_tokenizer\")\n",
    "enru_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-Russian_tokenizer\")\n",
    "enfi_tokenizer = AutoTokenizer.from_pretrained(\"../mt_tokenizers/mt5-small_English-Finnish_tokenizer\")\n",
    "#if args.tokenizer != args.model:\n",
    "model_tokenizer_vocab = model_tokenizer.get_vocab()\n",
    "ende_tokenizer_vocab = ende_tokenizer.get_vocab()\n",
    "enru_tokenizer_vocab = enru_tokenizer.get_vocab()\n",
    "enfi_tokenizer_vocab = enfi_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/German/dev-small\"\n",
    "de_pd = pd.read_csv(de_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "de_dataset = Dataset.from_pandas(de_pd)\n",
    "\n",
    "ru_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/Russian/dev-small\"\n",
    "ru_pd = pd.read_csv(ru_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "ru_dataset = Dataset.from_pandas(ru_pd)\n",
    "\n",
    "fi_path = \"/mmfs1/gscratch/ark/knylund/bad-pair-encoding/paracrawl_data/Finnish/dev-small\"\n",
    "fi_pd = pd.read_csv(fi_path, sep=\"\\t\", header=None, on_bad_lines=\"skip\", engine=\"python\")\n",
    "fi_dataset = Dataset.from_pandas(fi_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      Michael Ruse (2014), Philosophy of Science 81(...   \n",
      "1            Category: Category Member Since: 2009-10-19   \n",
      "2         Find your way through the world and to Gerard.   \n",
      "3      Nawang hasn’t written anything in his profile ...   \n",
      "4      Product details Tomatin three bridges - Ask a ...   \n",
      "...                                                  ...   \n",
      "24240  Wilde & Partner offers customized services to ...   \n",
      "24241  It is perfect for indoor and outdoor wear. You...   \n",
      "24242  Step 2: Install and launch 5K Player free YouT...   \n",
      "24243  I can’t really do much about it, but it helps ...   \n",
      "24244  The 10 best mechanical engineering teachers in...   \n",
      "\n",
      "                                                       1  \n",
      "0      Zeitschrift für philosophische Forschung 69 (2...  \n",
      "1           Erobere den ersten Mitglied seit: 2012-03-04  \n",
      "2      Durchlaufe die Welt und finde deinen Weg zum G...  \n",
      "3       Nisha hat noch nichts in her Profil geschrieben.  \n",
      "4      Produktdetails Mike Marshall & The Turtle Isla...  \n",
      "...                                                  ...  \n",
      "24240  Wilde & Partner Communications betreut seine K...  \n",
      "24241  Sie eignet sich sowohl als In- wie Outdoorjack...  \n",
      "24242  Schritt 2: Installieren und starten Sie den ko...  \n",
      "24243  Ich kann nicht wirklich viel dagegen tun, aber...  \n",
      "24244  10 besten Maschinenbau-Lehrer(n) in Otterbach,...  \n",
      "\n",
      "[24245 rows x 2 columns]\n",
      "Dataset({\n",
      "    features: ['0', '1'],\n",
      "    num_rows: 24338\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(de_pd)\n",
    "print(fi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193289\n",
      "189749\n",
      "198776\n"
     ]
    }
   ],
   "source": [
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(ende_tokenizer_vocab.keys()))))\n",
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(enru_tokenizer_vocab.keys()))))\n",
    "print(len(set(model_tokenizer_vocab.keys()).difference(set(enfi_tokenizer_vocab.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Kommt wieder, wann immer ihr wollt.“ Wow, wir waren wirklich sehr stolz auf dieses Feedback.\n",
      "['▁Komm', 't', '▁wieder', ',', '▁wann', '▁immer', '▁ihr', '▁wo', 'll', 't', '.', '“', '▁Wow', ',', '▁wir', '▁waren', '▁wirklich', '▁sehr', '▁stolz', '▁auf', '▁diese', 's', '▁Feedback', '.']\n",
      "['▁Komm', 't', '▁wieder', ',', '▁wann', '▁immer', '▁ihr', '▁wo', 'll', 't', '.', '“', '▁Wow', ',', '▁wir', '▁waren', '▁wirklich', '▁sehr', '▁stolz', '▁auf', '▁diese', 's', '▁Feedback', '.']\n"
     ]
    }
   ],
   "source": [
    "example_num = 115\n",
    "\n",
    "print(\"Sample: \" + de_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(de_dataset[example_num][\"1\"]))\n",
    "de_tokens = ende_tokenizer.tokenize(de_dataset[example_num][\"1\"])\n",
    "#print(f\"German-specific tokenization: {de_tokens}\")\n",
    "#print(de_tokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(de_tokens)):\n",
    "    if de_tokens[i] not in model_tokenizer_vocab:\n",
    "        def_de_tok = model_tokenizer.tokenize(de_tokens[i])\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if de_tokens[i][0] != '▁' and def_de_tok[0] == '▁':\n",
    "            def_de_tok = def_de_tok[1:]\n",
    "        elif def_de_tok[0][0] == '▁' and re_tokenized[-1] == '▁':\n",
    "            del re_tokenized[-1]\n",
    "        re_tokenized += def_de_tok\n",
    "    else:\n",
    "        re_tokenized.append(de_tokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "#print(f\"German and then default tokenized: {re_tokenized}\")\n",
    "#print(preprocess(dev_dataset[example_num], model_tokenizer))\n",
    "#print(preprocess(dev_dataset[example_num], ende_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Persoonallinen loft-asunto järven rannalla\n",
      "['▁Perso', 'on', 'allinen', '▁loft', '-', 'asunto', '▁', 'järven', '▁ra', 'nnalla']\n",
      "['▁Persoonalli', 'nen', '▁loft', '-asunto', '▁järven', '▁rannalla']\n",
      "['▁Perso', 'on', 'alli', 'nen', '▁loft', '-', 'asunto', '▁', 'järven', '▁ra', 'nnalla']\n",
      "[72453, 444, 138523, 58615, 264, 180575, 259, 145397, 1101, 212252, 1]\n",
      "[173450, 151, 23559, 111765, 23827, 5603, 1]\n",
      "[72453, 444, 43327, 3358, 58615, 264, 180575, 259, 145397, 1101, 212252, 1]\n"
     ]
    }
   ],
   "source": [
    "example_num = 2299\n",
    "\n",
    "print(\"Sample: \" + fi_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(fi_dataset[example_num][\"1\"]))\n",
    "fi_tokens = enfi_tokenizer.tokenize(fi_dataset[example_num][\"1\"])\n",
    "print(fi_tokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(fi_tokens)):\n",
    "    if fi_tokens[i] not in model_tokenizer_vocab:\n",
    "        def_fi_tok = model_tokenizer.tokenize(fi_tokens[i], add_special_tokens=False)\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if len(def_fi_tok) > 0:\n",
    "            if fi_tokens[i][0] != '▁' and def_fi_tok[0] == '▁':\n",
    "                def_fi_tok = def_fi_tok[1:]\n",
    "            elif def_fi_tok[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                del re_tokenized[-1]\n",
    "            re_tokenized += def_fi_tok\n",
    "    else:\n",
    "        re_tokenized.append(fi_tokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "print(preprocess(fi_dataset[example_num], model_tokenizer)[\"labels\"])\n",
    "print(preprocess(fi_dataset[example_num], enfi_tokenizer)[\"labels\"])\n",
    "print(model_tokenizer.convert_tokens_to_ids(re_tokenized) + [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Снять краску с помощью очистителя.\n",
      "['▁С', 'нять', '▁крас', 'ку', '▁с', '▁помощ', 'ью', '▁', 'очист', 'ителя', '.']\n",
      "['▁С', 'нять', '▁', 'краску', '▁с', '▁помощью', '▁', 'очистителя', '.']\n",
      "['▁С', 'нять', '▁крас', 'ку', '▁с', '▁помощ', 'ью', '▁', 'очист', 'ителя', '.']\n",
      "{'input_ids': [37194, 5413, 288, 20567, 267, 14572, 4368, 281, 314, 259, 4342, 259, 98668, 259, 45525, 11684, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [688, 20593, 34127, 989, 388, 8465, 4722, 259, 70857, 48101, 260, 1]}\n",
      "{'input_ids': [22130, 754, 12, 2388, 18, 8688, 1080, 15, 428, 3, 466, 3, 14692, 2911, 7549, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3, 2, 249753, 2, 3, 2, 3, 2, 3, 247617, 2, 3, 2, 249534, 2, 249534, 2, 249685, 249753, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_num = 9769\n",
    "\n",
    "print(\"Sample: \" + ru_dataset[example_num][\"1\"])\n",
    "#print(f'Default tokenization: {model_tokenizer.tokenize(dev_dataset[example_num][\"1\"])}')\n",
    "print(model_tokenizer.tokenize(ru_dataset[example_num][\"1\"]))\n",
    "fi_tokens = enru_tokenizer.tokenize(ru_dataset[example_num][\"1\"])\n",
    "print(fi_tokens)\n",
    "\n",
    "re_tokenized = []\n",
    "for i in range(len(fi_tokens)):\n",
    "    if fi_tokens[i] not in model_tokenizer_vocab:\n",
    "        def_fi_tok = model_tokenizer.tokenize(fi_tokens[i], add_special_tokens=False)\n",
    "        #print(def_de_tok, de_tokens[i])\n",
    "        if len(def_fi_tok) > 0:\n",
    "            if fi_tokens[i][0] != '▁' and def_fi_tok[0] == '▁':\n",
    "                def_fi_tok = def_fi_tok[1:]\n",
    "            elif def_fi_tok[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                del re_tokenized[-1]\n",
    "            re_tokenized += def_fi_tok\n",
    "    else:\n",
    "        re_tokenized.append(fi_tokens[i])\n",
    "\n",
    "print(re_tokenized)\n",
    "print(preprocess(ru_dataset[example_num], model_tokenizer))\n",
    "print(preprocess(ru_dataset[example_num], enfi_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Как', '▁правил', 'о', ',', '▁', 'наши', '▁', 'соо', 'течественн', 'ики', '▁хот', 'ят', '▁у', 'слыш', 'ать', '▁', 'еще', '▁одно', '▁мне', 'ние', '▁', 'авторитет', 'ного', '▁врач', 'а', ',', '▁пере', 'под', 'тверди', 'ть', '▁диагно', 'з', ':', '▁«', 'К', 'стати', ',', '▁в', '▁Германи', 'и', '▁за', 'част', 'ую', '▁с', 'мяг', 'чают', '▁диагно', 'з', ',', '▁предлага', 'ют', '▁мало', 'ин', 'ваз', 'ивное', '▁', 'лечение', '.']\n"
     ]
    }
   ],
   "source": [
    "print(model_tokenizer.tokenize(ru_dataset[example_num][\"1\"], max_length=128, truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize(tokens, new_tokenizer, new_tokenizer_vocab):\n",
    "    re_tokenized = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in new_tokenizer_vocab:\n",
    "            #print(tokens[i])\n",
    "            new_toks = new_tokenizer.tokenize(tokens[i], add_special_tokens=False)\n",
    "            #print(def_de_tok, de_tokens[i])\n",
    "            if len(new_toks) > 0:\n",
    "                if tokens[i][0] != '▁' and new_toks[0] == '▁':\n",
    "                    new_toks = new_toks[1:]\n",
    "                elif new_toks[0][0] == '▁' and len(re_tokenized) > 0 and re_tokenized[-1] == '▁':\n",
    "                    del re_tokenized[-1]\n",
    "                re_tokenized += new_toks\n",
    "        else:\n",
    "            re_tokenized.append(tokens[i])\n",
    "    re_tokenized = (new_tokenizer.convert_tokens_to_ids(re_tokenized) + [1])\n",
    "    return re_tokenized\n",
    "\n",
    "\n",
    "def lang_preprocess(sample):\n",
    "    inputs = prefix + str(sample[\"0\"])\n",
    "    targets = str(sample[\"1\"])\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"input_ids\"] = ende_tokenizer.tokenize(inputs, max_length=128, truncation=True)\n",
    "    model_inputs[\"input_ids\"] = retokenize(model_inputs[\"input_ids\"], model_tokenizer, set(model_tokenizer.get_vocab().keys()))\n",
    "    labels = model_tokenizer.tokenize(targets, max_length=128, truncation=True)\n",
    "    labels = retokenize(labels, model_tokenizer, model_tokenizer_vocab)\n",
    "    model_inputs[\"attention_mask\"] = ([1] * len(model_inputs[\"input_ids\"]))\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    #print(model_inputs)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [37194, 5413, 288, 20567, 267, 642, 262, 75443, 287, 1546, 3415, 304, 26594, 259, 108880, 261, 287, 259, 57611, 2178, 259, 53567, 2178, 259, 128868, 511, 19532, 259, 73433, 263, 15848, 287, 259, 54148, 23317, 259, 4964, 259, 159713, 305, 10385, 332, 259, 4609, 151738, 36732, 14679, 2178, 3399, 16185, 5233, 5927, 3826, 46213, 21511, 345, 514, 381, 403, 14181, 288, 259, 38195, 26594, 484, 264, 204941, 265, 25997, 305, 10239, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 22496, 259, 14416, 21638, 262, 6271, 286, 68041, 44050, 270, 309, 259, 57611, 2178, 259, 53567, 2178, 259, 106177, 511, 489, 264, 220805, 68041, 66934, 259, 45200, 272, 398, 42951, 9933, 259, 112444, 795, 259, 217071, 472, 58147, 749, 36732, 14679, 2178, 3399, 90849, 956, 67693, 119787, 3399, 264, 638, 50219, 28333, 472, 381, 403, 264, 174218, 956, 35834, 115806, 30966, 472, 259, 184667, 260, 1]}\n",
      "{'input_ids': [37194, 5413, 288, 20567, 267, 415, 294, 75443, 287, 1546, 259, 295, 262, 304, 26594, 259, 108880, 261, 287, 259, 57611, 2178, 259, 53567, 2178, 774, 5388, 511, 19532, 259, 73433, 263, 15848, 287, 259, 54148, 23317, 259, 4964, 259, 159713, 305, 10385, 332, 527, 270, 151738, 36732, 14679, 2178, 3399, 16185, 5233, 5927, 3826, 46213, 21511, 345, 514, 381, 403, 14181, 288, 259, 38195, 26594, 484, 264, 12854, 265, 25997, 305, 10239, 260, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 22496, 259, 14416, 21638, 262, 6271, 286, 68041, 44050, 270, 309, 259, 57611, 2178, 259, 53567, 2178, 259, 106177, 511, 489, 264, 220805, 68041, 66934, 259, 45200, 272, 398, 42951, 9933, 259, 112444, 795, 259, 217071, 472, 58147, 749, 36732, 14679, 2178, 3399, 90849, 956, 67693, 119787, 3399, 264, 638, 50219, 28333, 472, 381, 403, 264, 174218, 956, 35834, 115806, 30966, 472, 259, 184667, 260, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_num = 7\n",
    "print(preprocess(de_dataset[example_num], model_tokenizer))\n",
    "print(lang_preprocess(de_dataset[example_num]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
